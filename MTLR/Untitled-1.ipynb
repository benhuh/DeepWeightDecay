{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "from torch.nn.utils.parametrizations import orthogonal\n",
    "import torch.nn.utils.parametrize as P\n",
    "import torch.nn as nn\n",
    "from measurements2 import get_surrogate_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, n_x, n_t, n_f, n_k, same_x = False):\n",
    "        super().__init__()\n",
    "        orth_linear = orthogonal(torch.nn.Linear(n_x, n_f))\n",
    "        self.U = orth_linear.weight.data\n",
    "        self.A = torch.randn(n_t, 1, n_f)\n",
    "        \n",
    "        self.X = torch.randn(1, n_x, n_k) if same_x else torch.randn(n_t, n_x, n_k)\n",
    "        self.Y = self.A @ self.U @ self.X\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.X, self.Y\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W_list = nn.ParameterList([nn.Parameter(torch.randn(n_x,n_x)) for i in range(d-1)])\n",
    "        self.W_list.append(nn.Parameter(torch.randn(n_h,n_x)))\n",
    "        self.C = nn.Parameter(torch.randn(n_t, 1, n_h))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.C @ self.weight @ X\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        W = None\n",
    "        for W_ in self.W_list:\n",
    "            W = W_ if W is None else  W_ @ W\n",
    "        return W\n",
    "        \n",
    "    def eval(self, X, target):\n",
    "        out = self.forward(X)\n",
    "        # print(out.shape, target.shape)\n",
    "        loss = nn.MSELoss()(out, target)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        optim.zero_grad()\n",
    "        # out = model(X)\n",
    "        loss = model.eval(X,Y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width = 'wide'\n",
    "width = 'narrow'\n",
    "\n",
    "n_x = 36\n",
    "n_f = 4 #Feature dim\n",
    "n_k = 6 #12 # batch-size\n",
    "n_t = 64# 28 #20 #8  # num task\n",
    "n_h = n_f if width == 'narrow' else n_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = 1\n",
    "same_x = False\n",
    "# same_x = True\n",
    "\n",
    "teacher = Teacher (n_x, n_t, n_f, n_k, same_x = same_x)\n",
    "X, Y = teacher.data\n",
    "model = Model(d=d)\n",
    "\n",
    "\n",
    "# p_list = list(model.parameters())\n",
    "# print(X.shape, Y.shape)\n",
    "# print([p.shape for p in p_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 147.41111755371094\n",
      "1 147.24412536621094\n",
      "2 146.927490234375\n",
      "3 146.4771728515625\n",
      "4 145.90806579589844\n",
      "5 145.2337188720703\n",
      "6 144.4666748046875\n",
      "7 143.61834716796875\n",
      "8 142.69918823242188\n",
      "9 141.7186279296875\n",
      "10 140.685302734375\n",
      "11 139.60694885253906\n",
      "12 138.49063110351562\n",
      "13 137.3426055908203\n",
      "14 136.16859436035156\n",
      "15 134.9736785888672\n",
      "16 133.76239013671875\n",
      "17 132.5388641357422\n",
      "18 131.30665588378906\n",
      "19 130.06898498535156\n",
      "20 128.82875061035156\n",
      "21 127.58841705322266\n",
      "22 126.35025024414062\n",
      "23 125.11617279052734\n",
      "24 123.88790893554688\n",
      "25 122.66696166992188\n",
      "26 121.45459747314453\n",
      "27 120.251953125\n",
      "28 119.05997467041016\n",
      "29 117.87950897216797\n",
      "30 116.7112045288086\n",
      "31 115.55567169189453\n",
      "32 114.41339874267578\n",
      "33 113.2847671508789\n",
      "34 112.17005157470703\n",
      "35 111.06957244873047\n",
      "36 109.98343658447266\n",
      "37 108.91180419921875\n",
      "38 107.85472869873047\n",
      "39 106.81226348876953\n",
      "40 105.78441619873047\n",
      "41 104.7711181640625\n",
      "42 103.77233123779297\n",
      "43 102.78793334960938\n",
      "44 101.81786346435547\n",
      "45 100.8619384765625\n",
      "46 99.9200668334961\n",
      "47 98.9920425415039\n",
      "48 98.07775115966797\n",
      "49 97.17697143554688\n",
      "50 96.2895278930664\n",
      "51 95.41522979736328\n",
      "52 94.55389404296875\n",
      "53 93.70529174804688\n",
      "54 92.86925506591797\n",
      "55 92.0455551147461\n",
      "56 91.2339859008789\n",
      "57 90.43435668945312\n",
      "58 89.64644622802734\n",
      "59 88.87005615234375\n",
      "60 88.10498809814453\n",
      "61 87.35103607177734\n",
      "62 86.60799407958984\n",
      "63 85.87566375732422\n",
      "64 85.15384674072266\n",
      "65 84.4423599243164\n",
      "66 83.74102020263672\n",
      "67 83.04961395263672\n",
      "68 82.36797332763672\n",
      "69 81.6958999633789\n",
      "70 81.03325653076172\n",
      "71 80.37981414794922\n",
      "72 79.73544311523438\n",
      "73 79.09994506835938\n",
      "74 78.47318267822266\n",
      "75 77.85497283935547\n",
      "76 77.24517059326172\n",
      "77 76.64360809326172\n",
      "78 76.05013275146484\n",
      "79 75.46460723876953\n",
      "80 74.88687896728516\n",
      "81 74.31681060791016\n",
      "82 73.75426483154297\n",
      "83 73.19908142089844\n",
      "84 72.6511459350586\n",
      "85 72.1103286743164\n",
      "86 71.57649993896484\n",
      "87 71.04952239990234\n",
      "88 70.52928924560547\n",
      "89 70.01567077636719\n",
      "90 69.50855255126953\n",
      "91 69.00782012939453\n",
      "92 68.51336669921875\n",
      "93 68.02507781982422\n",
      "94 67.54283905029297\n",
      "95 67.0665512084961\n",
      "96 66.59612274169922\n",
      "97 66.13142395019531\n",
      "98 65.67239379882812\n",
      "99 65.21890258789062\n",
      "100 64.77088928222656\n",
      "101 64.3282241821289\n",
      "102 63.890838623046875\n",
      "103 63.458648681640625\n",
      "104 63.03156661987305\n",
      "105 62.6094856262207\n",
      "106 62.19236373901367\n",
      "107 61.78007125854492\n",
      "108 61.372562408447266\n",
      "109 60.969757080078125\n",
      "110 60.57156753540039\n",
      "111 60.17792892456055\n",
      "112 59.78875732421875\n",
      "113 59.403995513916016\n",
      "114 59.023563385009766\n",
      "115 58.64738845825195\n",
      "116 58.275421142578125\n",
      "117 57.9075813293457\n",
      "118 57.54380416870117\n",
      "119 57.18404006958008\n",
      "120 56.82820510864258\n",
      "121 56.47625732421875\n",
      "122 56.12813186645508\n",
      "123 55.78376770019531\n",
      "124 55.443111419677734\n",
      "125 55.10609817504883\n",
      "126 54.77268600463867\n",
      "127 54.44282150268555\n",
      "128 54.116424560546875\n",
      "129 53.79347610473633\n",
      "130 53.47391891479492\n",
      "131 53.157684326171875\n",
      "132 52.84474563598633\n",
      "133 52.53504943847656\n",
      "134 52.22853469848633\n",
      "135 51.9251708984375\n",
      "136 51.62490463256836\n",
      "137 51.327693939208984\n",
      "138 51.03349685668945\n",
      "139 50.74226760864258\n",
      "140 50.4539680480957\n",
      "141 50.16855239868164\n",
      "142 49.885986328125\n",
      "143 49.60621643066406\n",
      "144 49.3292121887207\n",
      "145 49.0549430847168\n",
      "146 48.78336715698242\n",
      "147 48.51443099975586\n",
      "148 48.24811935424805\n",
      "149 47.9843864440918\n",
      "150 47.72319412231445\n",
      "151 47.46452331542969\n",
      "152 47.20832443237305\n",
      "153 46.95457077026367\n",
      "154 46.70322036743164\n",
      "155 46.45425796508789\n",
      "156 46.2076416015625\n",
      "157 45.96333694458008\n",
      "158 45.7213249206543\n",
      "159 45.481563568115234\n",
      "160 45.2440299987793\n",
      "161 45.00869369506836\n",
      "162 44.7755241394043\n",
      "163 44.54449462890625\n",
      "164 44.315582275390625\n",
      "165 44.088748931884766\n",
      "166 43.863983154296875\n",
      "167 43.641239166259766\n",
      "168 43.42051315307617\n",
      "169 43.20176315307617\n",
      "170 42.98496627807617\n",
      "171 42.770111083984375\n",
      "172 42.55715560913086\n",
      "173 42.34608459472656\n",
      "174 42.136871337890625\n",
      "175 41.929508209228516\n",
      "176 41.72394943237305\n",
      "177 41.52018356323242\n",
      "178 41.31819152832031\n",
      "179 41.11794662475586\n",
      "180 40.919429779052734\n",
      "181 40.722625732421875\n",
      "182 40.527496337890625\n",
      "183 40.334041595458984\n",
      "184 40.14222717285156\n",
      "185 39.95204544067383\n",
      "186 39.76346969604492\n",
      "187 39.57647705078125\n",
      "188 39.39105224609375\n",
      "189 39.207183837890625\n",
      "190 39.024845123291016\n",
      "191 38.84401321411133\n",
      "192 38.66469192504883\n",
      "193 38.48684310913086\n",
      "194 38.31045913696289\n",
      "195 38.13551330566406\n",
      "196 37.96200180053711\n",
      "197 37.78989791870117\n",
      "198 37.619197845458984\n",
      "199 37.44987106323242\n",
      "200 37.28191375732422\n",
      "201 37.115299224853516\n",
      "202 36.95001983642578\n",
      "203 36.78606033325195\n",
      "204 36.62340545654297\n",
      "205 36.46204376220703\n",
      "206 36.30195236206055\n",
      "207 36.143123626708984\n",
      "208 35.98554611206055\n",
      "209 35.829193115234375\n",
      "210 35.6740608215332\n",
      "211 35.520137786865234\n",
      "212 35.367408752441406\n",
      "213 35.21585464477539\n",
      "214 35.06547546386719\n",
      "215 34.9162483215332\n",
      "216 34.768157958984375\n",
      "217 34.6212043762207\n",
      "218 34.47536087036133\n",
      "219 34.33062744140625\n",
      "220 34.186988830566406\n",
      "221 34.044429779052734\n",
      "222 33.9029426574707\n",
      "223 33.762516021728516\n",
      "224 33.623138427734375\n",
      "225 33.48479461669922\n",
      "226 33.347476959228516\n",
      "227 33.211181640625\n",
      "228 33.07588577270508\n",
      "229 32.941585540771484\n",
      "230 32.80827331542969\n",
      "231 32.67593002319336\n",
      "232 32.544551849365234\n",
      "233 32.41413497924805\n",
      "234 32.28465270996094\n",
      "235 32.15610885620117\n",
      "236 32.02849197387695\n",
      "237 31.90179443359375\n",
      "238 31.7759952545166\n",
      "239 31.65110206604004\n",
      "240 31.5270938873291\n",
      "241 31.403966903686523\n",
      "242 31.281713485717773\n",
      "243 31.16031837463379\n",
      "244 31.039772033691406\n",
      "245 30.920076370239258\n",
      "246 30.80122184753418\n",
      "247 30.683189392089844\n",
      "248 30.565980911254883\n",
      "249 30.449586868286133\n",
      "250 30.33399200439453\n",
      "251 30.219200134277344\n",
      "252 30.105192184448242\n",
      "253 29.991968154907227\n",
      "254 29.8795166015625\n",
      "255 29.767831802368164\n",
      "256 29.656911849975586\n",
      "257 29.54673194885254\n",
      "258 29.437307357788086\n",
      "259 29.32861328125\n",
      "260 29.220657348632812\n",
      "261 29.113418579101562\n",
      "262 29.00689697265625\n",
      "263 28.901086807250977\n",
      "264 28.795974731445312\n",
      "265 28.69156837463379\n",
      "266 28.587846755981445\n",
      "267 28.48480796813965\n",
      "268 28.3824520111084\n",
      "269 28.28076171875\n",
      "270 28.179738998413086\n",
      "271 28.079374313354492\n",
      "272 27.97966194152832\n",
      "273 27.880599975585938\n",
      "274 27.78217315673828\n",
      "275 27.684385299682617\n",
      "276 27.58722496032715\n",
      "277 27.490690231323242\n",
      "278 27.3947696685791\n",
      "279 27.299463272094727\n",
      "280 27.20476531982422\n",
      "281 27.11066436767578\n",
      "282 27.01716423034668\n",
      "283 26.92424964904785\n",
      "284 26.831926345825195\n",
      "285 26.74017906188965\n",
      "286 26.649009704589844\n",
      "287 26.55840492248535\n",
      "288 26.468368530273438\n",
      "289 26.378890991210938\n",
      "290 26.28997039794922\n",
      "291 26.20159912109375\n",
      "292 26.113771438598633\n",
      "293 26.026487350463867\n",
      "294 25.93973731994629\n",
      "295 25.8535213470459\n",
      "296 25.7678279876709\n",
      "297 25.682655334472656\n",
      "298 25.598005294799805\n",
      "299 25.513864517211914\n"
     ]
    }
   ],
   "source": [
    "train(epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0764, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W=model.weight\n",
    "U=teacher.U\n",
    "sl=get_surrogate_loss2(W,U,eps=0)\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0764, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "u, sig, v = W.svd()\n",
    "W_normalized = u @ v.T\n",
    "\n",
    "# print(W_normalized@W_normalized.T, U@U.T)\n",
    "\n",
    "err = W_normalized.T@W_normalized - U.T@U\n",
    "surrogate_loss = (err.norm()**2/  err.numel()).sqrt()\n",
    "print(surrogate_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = teacher.data\n",
    "# out = model(X)\n",
    "loss = model.eval(X,Y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(n_t, n_x, n_k)\n",
    "a = U @ X\n",
    "o = C @ a\n",
    "\n",
    "CU = C@U\n",
    "# print(a.shape)\n",
    "# print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 36]) torch.Size([8, 1, 4]) torch.Size([4, 36])\n",
      "288 32 144\n"
     ]
    }
   ],
   "source": [
    "print(CU.shape, C.shape, U.shape)\n",
    "print(CU.numel(), C.numel(), U.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bfb7d1208254fd3f86303a570996ee6b514cbce3f3356396ab091f22acee806"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('async3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
